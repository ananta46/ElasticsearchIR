{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "from scipy import sparse\n",
    "\n",
    "# Web/API imports\n",
    "from elasticsearch import Elasticsearch\n",
    "from flask import Flask, request, render_template\n",
    "\n",
    "# ML/Feature extraction imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Parquet handling\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFIndexer:\n",
    "    def __init__(self, is_reset=False):\n",
    "        self.crawled_folder = Path(Path().absolute()) / \"crawled/\"\n",
    "        self.stored_file = 'src/resource/tfidf_indexer.pkl'  # Changed filename to avoid conflict\n",
    "        self.vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "        \n",
    "        # Load the cached index if it exists and is_reset is False\n",
    "        if not is_reset and os.path.isfile(self.stored_file):\n",
    "            with open(self.stored_file, \"rb\") as f:\n",
    "                cached_dict = pickle.load(f)\n",
    "            self.__dict__.update(cached_dict)\n",
    "        else:\n",
    "            self.run_indexer()\n",
    "            \n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Lowercase text\n",
    "        text = text.lower()\n",
    "        # Remove punctuation and special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def run_indexer(self):\n",
    "        documents = []\n",
    "\n",
    "        # Load all .txt files from crawled folder\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            # Skip files that aren't .txt or contain 'pdf' in the URL\n",
    "            if not file.endswith(\".txt\"):\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(self.crawled_folder, file)\n",
    "\n",
    "            # Read the file with UTF-8 encoding and handle errors\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                try:\n",
    "                    j = json.load(f)\n",
    "                    # Skip entries with PDF URLs or missing required fields\n",
    "                    if ('url' in j and '.pdf' in j['url'].lower()) or \\\n",
    "                    'title' not in j or 'text' not in j:\n",
    "                        print(f\"Skipped file {file}: PDF URL or missing required fields\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Additional check for binary-looking content\n",
    "                    if any(ord(c) < 32 and c not in '\\n\\r\\t' for c in j.get('text', '')):\n",
    "                        print(f\"Skipped file {file}: Contains binary content\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Add filename to the document\n",
    "                    j['filename'] = file\n",
    "                    \n",
    "                    # Preprocess title and text\n",
    "                    j['title'] = self.preprocess_text(j['title'])\n",
    "                    j['text'] = self.preprocess_text(j['text'])\n",
    "                    \n",
    "                    # Skip if content is too short after preprocessing\n",
    "                    if len(j['text']) < 50:  # Adjust minimum length as needed\n",
    "                        print(f\"Skipped file {file}: Content too short\")\n",
    "                        continue\n",
    "                        \n",
    "                    documents.append(j)\n",
    "                    \n",
    "                except:\n",
    "                    print(f\"Error reading {file}\")\n",
    "\n",
    "        if not documents:\n",
    "            raise ValueError(\"No valid documents found. Ensure crawled/ folder contains valid .txt files.\")\n",
    "\n",
    "        # Create DataFrame and prepare corpus\n",
    "        self.documents = pd.DataFrame.from_dict(documents)\n",
    "        self.corpus = self.documents.apply(lambda s: ' '.join(s[['title', 'text']]), axis=1).tolist()\n",
    "\n",
    "        if not self.corpus:\n",
    "            raise ValueError(\"Corpus is empty. Ensure documents contain 'title' and 'text' fields.\")\n",
    "\n",
    "        # Fit TF-IDF vectorizer on corpus\n",
    "        self.vectorizer.fit(self.corpus)\n",
    "        self.tfidf_matrix = self.vectorizer.transform(self.corpus)\n",
    "\n",
    "        # Ensure directory exists before saving\n",
    "        os.makedirs(os.path.dirname(self.stored_file), exist_ok=True)\n",
    "\n",
    "        # Save the final processed data\n",
    "        with open(self.stored_file, \"wb\") as f:\n",
    "            pickle.dump(self.__dict__, f)\n",
    "\n",
    "    def search_query(self, query, top_n=5):\n",
    "        # Preprocess query\n",
    "        query = self.preprocess_text(query)\n",
    "        \n",
    "        # Transform query to TF-IDF space\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate TF-IDF similarity scores\n",
    "        tfidf_scores = (self.tfidf_matrix @ query_vector.T).toarray().flatten()\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = self.documents.copy()\n",
    "        results_df['score'] = tfidf_scores\n",
    "        \n",
    "        # Get top results\n",
    "        top_results = results_df.nlargest(top_n, 'score')[\n",
    "            ['url', 'title', 'text', 'filename', 'score']  # Added filename to output\n",
    "        ]\n",
    "        \n",
    "        return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_text(filename, url=None):\n",
    "    \"\"\"Get original text from crawled file based on filename or URL.\"\"\"\n",
    "    try:\n",
    "        # First try with filename\n",
    "        if pd.notna(filename):\n",
    "            file_path = os.path.join('crawled', filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return data['text']\n",
    "        \n",
    "        # If filename is NaN or file not found, try searching by URL\n",
    "        if url:\n",
    "            files = [f for f in os.listdir('crawled') if f.endswith('.txt')]\n",
    "            for fname in files:\n",
    "                try:\n",
    "                    with open(os.path.join('crawled', fname), 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "                        if data['url'] == url or (url in data.get('url_lists', [])):\n",
    "                            return data['text']\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text from {filename} or {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_relevant_sentences(text, query, max_length=200):\n",
    "    \"\"\"Get Google-like snippet with query terms highlighted in context.\"\"\"\n",
    "    if not text or not query:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Normalize text and query\n",
    "        text = text.replace('\\n', ' ').strip()\n",
    "        query_terms = [term.lower() for term in query.split()]\n",
    "        \n",
    "        # Find the first occurrence of any query term\n",
    "        text_lower = text.lower()\n",
    "        positions = []\n",
    "        \n",
    "        # Find all occurrences of query terms\n",
    "        for term in query_terms:\n",
    "            pos = text_lower.find(term)\n",
    "            if pos >= 0:\n",
    "                positions.append(pos)\n",
    "        \n",
    "        if not positions:\n",
    "            return text[:max_length] + \"...\" if len(text) > max_length else text\n",
    "        \n",
    "        # Get the first occurrence\n",
    "        start_pos = min(positions)\n",
    "        \n",
    "        # Find sentence boundaries\n",
    "        sentence_start = text.rfind('.', 0, start_pos)\n",
    "        if sentence_start == -1:\n",
    "            sentence_start = max(0, start_pos - 50)\n",
    "        else:\n",
    "            sentence_start += 1\n",
    "            \n",
    "        # Get enough context after the query term\n",
    "        end_pos = start_pos + max_length\n",
    "        sentence_end = text.find('.', end_pos)\n",
    "        if sentence_end == -1:\n",
    "            sentence_end = min(len(text), end_pos + 50)\n",
    "        \n",
    "        # Extract the relevant portion\n",
    "        snippet = text[sentence_start:sentence_end].strip()\n",
    "        \n",
    "        # If snippet is too long, trim it while keeping the query term\n",
    "        if len(snippet) > max_length:\n",
    "            # Ensure we keep the part with the query term\n",
    "            term_pos = min(positions) - sentence_start\n",
    "            if term_pos < max_length/2:\n",
    "                snippet = snippet[:max_length] + \"...\"\n",
    "            else:\n",
    "                start_offset = term_pos - int(max_length/3)\n",
    "                snippet = \"...\" + snippet[start_offset:start_offset + max_length] + \"...\"\n",
    "        \n",
    "        return snippet\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating snippet: {e}\")\n",
    "        return text[:max_length] if text else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tfidf_search(query_term, pr_weight=0.3):\n",
    "    \"\"\"Process TF-IDF search with PageRank combination but get text from ES.\"\"\"\n",
    "    start = time.time()\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    try:\n",
    "        # Get initial TF-IDF results\n",
    "        results_df = tfidf_indexer.search_query(query_term, top_n=100)\n",
    "        \n",
    "        # Get the nice formatted text from Elasticsearch for these URLs\n",
    "        es_results = app.es_client.search(\n",
    "            index='simple',\n",
    "            source_excludes=['url_lists'],\n",
    "            size=100,\n",
    "            query={\n",
    "                \"terms\": {\n",
    "                    \"url.keyword\": results_df['url'].tolist()\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create a mapping from URL to ES content\n",
    "        es_content = {\n",
    "            hit['_source']['url']: {\n",
    "                'title': hit['_source']['title'],\n",
    "                'text': hit['_source']['text'][:200],\n",
    "            }\n",
    "            for hit in es_results['hits']['hits']\n",
    "        }\n",
    "        \n",
    "        # Update the text content while keeping TF-IDF scores\n",
    "        results_df['title'] = results_df['url'].map(lambda x: es_content.get(x, {}).get('title', results_df.loc[results_df['url'] == x, 'title'].iloc[0]))\n",
    "        results_df['text'] = results_df['url'].map(lambda x: es_content.get(x, {}).get('text', results_df.loc[results_df['url'] == x, 'text'].iloc[0]))\n",
    "        \n",
    "        # Get relevant text using the nice ES text\n",
    "        results_df['relevant_text'] = results_df.apply(\n",
    "            lambda row: get_relevant_sentences(\n",
    "                get_original_text(row['filename']), \n",
    "                query_term\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Rest of your existing processing\n",
    "        pr_scores = pd.read_parquet('pagerank_results.parquet')\n",
    "        \n",
    "        # Normalize TF-IDF scores\n",
    "        results_df['tfidf_norm'] = (results_df['score'] - results_df['score'].min()) / \\\n",
    "                                 (results_df['score'].max() - results_df['score'].min())\n",
    "        \n",
    "        # Add normalized PageRank scores\n",
    "        results_df['pagerank'] = pr_scores.loc[results_df['url']]['score'].values\n",
    "        results_df['pagerank_norm'] = (results_df['pagerank'] - results_df['pagerank'].min()) / \\\n",
    "                                    (results_df['pagerank'].max() - results_df['pagerank'].min())\n",
    "        \n",
    "        # Combine scores with weight\n",
    "        results_df['final_score'] = (1 - pr_weight) * results_df['tfidf_norm'] + \\\n",
    "                                  pr_weight * results_df['pagerank_norm']\n",
    "        \n",
    "        # Resort by combined scores\n",
    "        results_df = results_df.nlargest(100, 'final_score')\n",
    "        \n",
    "        # Prepare final output\n",
    "        output_df = results_df[['url', 'title', 'text', 'relevant_text', 'final_score']]\n",
    "        output_df = output_df.rename(columns={'final_score': 'score'})\n",
    "        \n",
    "        response_object['total_hit'] = len(output_df)\n",
    "        response_object['results'] = output_df.to_dict('records')\n",
    "        response_object['elapse'] = time.time() - start\n",
    "        \n",
    "        return response_object\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_tfidf_search: {str(e)}\")\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = str(e)\n",
    "        return response_object\n",
    "def process_es_search(query_term, pr_weight=0.3):\n",
    "    \"\"\"Process Elasticsearch search with PageRank combination.\"\"\"\n",
    "    start = time.time()\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    try:\n",
    "        # Get BM25 results from Elasticsearch\n",
    "        results = app.es_client.search(\n",
    "            index='simple',\n",
    "            source_excludes=['url_lists'],\n",
    "            size=100,\n",
    "            query={\n",
    "                \"match\": {\n",
    "                    \"text\": query_term\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        results_df = pd.DataFrame([\n",
    "            {\n",
    "                'url': hit['_source']['url'],\n",
    "                'title': hit['_source']['title'],\n",
    "                'text': hit['_source']['text'][:200],\n",
    "                'es_score': hit['_score']\n",
    "            } \n",
    "            for hit in results['hits']['hits']\n",
    "        ])\n",
    "        \n",
    "        # Get existing files from crawled folder\n",
    "        crawled_files = {f for f in os.listdir('crawled') if f.endswith('.txt')}\n",
    "        \n",
    "        # Get filenames from TF-IDF indexer using URL as key\n",
    "        url_to_filename = dict(zip(tfidf_indexer.documents['url'], tfidf_indexer.documents['filename']))\n",
    "        results_df['filename'] = results_df['url'].map(url_to_filename)\n",
    "        \n",
    "        # Print debugging info for missing files\n",
    "        missing_files = results_df[results_df['filename'].isna()]\n",
    "        if not missing_files.empty:\n",
    "            print(\"\\nMissing files for URLs:\")\n",
    "            for url in missing_files['url']:\n",
    "                print(f\"No filename found for URL: {url}\")\n",
    "                \n",
    "        mismatched_files = results_df[\n",
    "            ~results_df['filename'].isna() & \n",
    "            ~results_df['filename'].isin(crawled_files)\n",
    "        ]\n",
    "        if not mismatched_files.empty:\n",
    "            print(\"\\nFiles not found in crawled folder:\")\n",
    "            for _, row in mismatched_files.iterrows():\n",
    "                print(f\"File {row['filename']} not found for URL: {row['url']}\")\n",
    "        \n",
    "        # Get relevant text\n",
    "        results_df['relevant_text'] = results_df.apply(\n",
    "            lambda row: get_relevant_sentences(\n",
    "                get_original_text(row['filename'], row['url']), \n",
    "                query_term\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Load PageRank scores\n",
    "        pr_scores = pd.read_parquet('pagerank_results.parquet')\n",
    "        \n",
    "        # Normalize Elasticsearch scores\n",
    "        results_df['es_norm'] = (results_df['es_score'] - results_df['es_score'].min()) / \\\n",
    "                               (results_df['es_score'].max() - results_df['es_score'].min())\n",
    "        \n",
    "        # Add normalized PageRank scores\n",
    "        results_df['pagerank'] = pr_scores.loc[results_df['url']]['score'].values\n",
    "        results_df['pagerank_norm'] = (results_df['pagerank'] - results_df['pagerank'].min()) / \\\n",
    "                                    (results_df['pagerank'].max() - results_df['pagerank'].min())\n",
    "        \n",
    "        # Combine scores with weight\n",
    "        results_df['final_score'] = (1 - pr_weight) * results_df['es_norm'] + \\\n",
    "                                  pr_weight * results_df['pagerank_norm']\n",
    "        \n",
    "        # Resort by combined scores\n",
    "        results_df = results_df.nlargest(100, 'final_score')\n",
    "        \n",
    "        # Prepare final output\n",
    "        output_df = results_df[['url', 'title', 'text', 'relevant_text', 'final_score']]\n",
    "        output_df = output_df.rename(columns={'final_score': 'score'})\n",
    "        \n",
    "        response_object['total_hit'] = len(output_df)\n",
    "        response_object['results'] = output_df.to_dict('records')\n",
    "        response_object['elapse'] = time.time() - start\n",
    "        \n",
    "        return response_object\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_es_search: {str(e)}\")\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = str(e)\n",
    "        return response_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask application setup\n",
    "from flask import render_template\n",
    "\n",
    "\n",
    "# Flask application setup\n",
    "app = Flask(__name__, static_folder='Frontend', template_folder='Frontend')\n",
    "\n",
    "# Initialize Elasticsearch client with your configurations\n",
    "app.es_client = Elasticsearch(\n",
    "    \"https://localhost:9200\",  # Your Elasticsearch host\n",
    "    basic_auth=(\"elastic\", \"+zu*TMbwCT-I9_fi3-L4\"),  # Your credentials\n",
    "    ca_certs=\"~/http_ca.crt\",  # Your certificate path\n",
    "    verify_certs=True  # Verify SSL certificates\n",
    ")\n",
    "\n",
    "# Initialize TF-IDF indexer\n",
    "tfidf_indexer = TFIDFIndexer(is_reset=False)\n",
    "\n",
    "# Root route to serve home.html\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template(\"home.html\")\n",
    "\n",
    "@app.route('/search_tfidf_pr', methods=['GET'])\n",
    "def search_tfidf_pr():\n",
    "    # Get query parameters\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term = argList['query'][0]\n",
    "    \n",
    "    # Convert weight from percentage to decimal (default 30%)\n",
    "    weight_percent = float(argList.get('weight', [30])[0])\n",
    "    pr_weight = weight_percent / 100\n",
    "    \n",
    "    # Return search results\n",
    "    return process_tfidf_search(query_term, pr_weight)\n",
    "\n",
    "@app.route('/search_es_pr', methods=['GET'])\n",
    "def search_es_pr():\n",
    "    # Get query parameters\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term = argList['query'][0]\n",
    "    \n",
    "    # Convert weight from percentage to decimal (default 30%)\n",
    "    weight_percent = float(argList.get('weight', [30])[0])\n",
    "    pr_weight = weight_percent / 100\n",
    "    \n",
    "    # Return search results\n",
    "    return process_es_search(query_term, pr_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
